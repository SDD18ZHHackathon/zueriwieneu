return(row.names(embed_mat)[idx])
}
pointer <- embedding_matrix['king',] - embedding_matrix['man',] + embedding_matrix['women',]
get_knn(pointer, embedding_matrix, n = 10)
# example 3: related words
get_knn(embedding_matrix['spain',], embedding_matrix, n = 10)
# example 3: related words
get_knn(embedding_matrix['spain',], embedding_matrix, n = 10, method = 'euclidean')
crossprod(c(1,2,3))
get_knn <- function(pointer, embed_mat, n = 5, method = 'cosine'){
if(method == 'euclidean'){
embed_mat <- sweep(embed_mat, 2, pointer, '-')
embed_mat <- embed_mat^2
distn <- sqrt(rowSums(embed_mat))
}else if(method == 'cosine'){
distn <- apply(embed_mat, 1, cosine_sim, pointer) * -1
}
# knn
idx <- head(order(distn), n)
return(row.names(embed_mat)[idx])
}
# example 3: related words
get_knn(embedding_matrix['spain',], embedding_matrix, n = 10, method = 'cosine')
pointer <- embedding_matrix['king',] - embedding_matrix['man',] + embedding_matrix['women',]
get_knn(pointer, embedding_matrix, n = 10)
text <- c('Das ist ein Test für Tokenization.'
, 'Neben tm gibt es auch text2vec, oder gensim for python.'
, 'ein 2-gram ist z.B. New York')
library(tm)
install.packages('tm')
library(tm)
install.packages('tm')
library(tm)
install.packages('tm')
library(tm)
install.packages('slam')
install.packages('tm')
library(tm)
text <- c('Das ist ein Test für Tokenization.'
, 'Neben tm gibt es auch text2vec, oder gensim for python.'
, 'ein 2-gram ist z.B. New York')
# clean text
text <- gsub('&nbsp;', ' ', text) # html space
text <- gsub('&amp;', 'and', text) # html 'and'
text <- gsub('[-/\n.]', ' ', text) # replace with whitespace, otherwise words get contracted
text <- gsub('[´]', '', text)
text <- gsub('[^[:graph:]|[:blank:]]', '', text)
old <- "šžþàáâãäåçèéêëìíîïðñòóôõöùúûüý"
new <- "szyaaaaaaceeeeiiiidnooooouuuuy"
text <- chartr(old, new, text)
text
# create corpus
my_docs <- tm::VectorSource(text)
my_corpus <- tm::Corpus(my_docs)
# transform documents
my_corpus <- tm::tm_map(my_corpus, enc2utf8)
my_corpus <- tm::tm_map(my_corpus, tm::removePunctuation)
my_corpus <- tm::tm_map(my_corpus, tm::removeNumbers)
my_corpus <- tm::tm_map(my_corpus, tm::stripWhitespace)
my_corpus <- tm::tm_map(my_corpus, tm::content_transformer(tolower))
my_corpus <- tm::tm_map(my_corpus, tm::removeWords, tm::stopwords('en'))
my_corpus
inspect(my_corpus)
my_corpus <- tm::Corpus(my_docs)
inspect(my_corpus)
my_corpus <- tm::tm_map(my_corpus, enc2utf8)
my_corpus <- tm::tm_map(my_corpus, tm::removePunctuation)
my_corpus <- tm::tm_map(my_corpus, tm::removeNumbers)
my_corpus <- tm::tm_map(my_corpus, tm::stripWhitespace)
my_corpus <- tm::tm_map(my_corpus, tm::content_transformer(tolower))
my_corpus <- tm::tm_map(my_corpus, tm::removeWords, tm::stopwords('en'))
my_corpus <- tm::tm_map(my_corpus, tm::stemDocument, language = 'en')
install.packages('SnowballC')
my_corpus <- tm::tm_map(my_corpus, tm::stemDocument, language = 'en')
# check corpus
my_corpus
inspect(my_corpus)
inspect(my_corpus[2])
# create term-document-matrix or document-term-matrix
term_doc_mat <- tm::TermDocumentMatrix(my_corpus)
term_doc_mat
findMostFreqTerms(term_doc_mat)
findFreqTerms(term_doc_mat,100) # show words that appear at least 100 times
findFreqTerms(term_doc_mat,2) # show words that appear at least 2 times (by document)
findMostFreqTerms(term_doc_mat)
?findMostFreqTerms
findMostFreqTerms(term_doc_mat, n = 2) # most frequent terms by document
term_doc_mat$dimnames
term_doc_mat$i
term_doc_mat$j
library(text2vec)
install.packages('text2vec')
text <- c('Das ist ein Test für Tokenization.'
, 'Neben tm gibt es auch text2vec, oder gensim for python.'
, 'ein 2-gram ist z.B. New York')
?tolower
library(text2vec)
?word_tokenizer
# define preprocession function and tokenizer
prep_fun = tolower
tok_fun = word_tokenizer
tokenizer = itoken(text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = labels, # here each document can be given a label
progressbar = FALSE)
vocab = create_vocabulary(tokenizer)
vocab
text <- c('Das ist ein Test für Tokenization.'
, 'Neben tm gibt es auch text2vec, oder gensim for python.'
, 'ein 2-gram ist z.B. New York oder ein ein')
tokenizer = itoken(text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = labels, # here each document can be given a label
progressbar = FALSE)
vocab = create_vocabulary(tokenizer)
vocab
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab)
vectorizer
# create document term matrix
dtm_train = create_dtm(it_train, vectorizer)
vocab
# create iterator function
iterator = itoken(text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = labels, # here each document can be given a label
progressbar = FALSE)
vocab = create_vocabulary(iterator)
vocab
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab)
# create document term matrix
dtm_train = create_dtm(iterator, vectorizer)
# create document term matrix
doc_term_mat = create_dtm(iterator, vectorizer)
doc_term_mat
as.matrix(doc_term_mat)
dim(doc_term_mat)
tm::stopwords()
vocab = create_vocabulary(iterator, stopwords = tm::stopwords())
vocab
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 2
, doc_proportion_min = 0.001
, doc_proportion_max = 0.5)
vocab_pruned
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 2
, doc_proportion_min = 0.001
, doc_proportion_max = 0.9)
vocab_pruned
vocab_pruned
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab)
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab_pruned)
vectorizer
# create document term matrix (sparse)
doc_term_mat = create_dtm(iterator, vectorizer)
doc_term_mat
as.matrix(doc_term_mat)
# create vocabulary (use stopwords from tm package)
vocab = create_vocabulary(iterator
, stopwords = tm::stopwords()
, ngram = c(1, 2))
vocab
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 2
, doc_proportion_min = 0.001
, doc_proportion_max = 0.9)
vocab_pruned
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab_pruned)
# create document term matrix (sparse)
doc_term_mat = create_dtm(iterator, vectorizer)
doc_term_mat
as.matrix(doc_term_mat)
# create tfidf matrix
tfidf_function <- TfIdf$new()
tfidf <- fit_transform(doc_term_mat, tfidf_function)
tfidf_function
tfidf
tfidf_function <- fit(doc_term_mat)
tfidf_function <- fit(doc_term_mat, tfidf_function)
?Collocations
# collocations
collocation_model <- Collocations$new()
collocation_model$fit(iterator, n_iter = 2) # repeats 2 times to get 3 word collocations
text <- readLines('~/Dropbox/Data_Projects/Fake_News/data/text_input.txt')
# define preprocession function and tokenizer
prep_fun = tolower
tok_fun = word_tokenizer
# create iterator function
iterator = itoken(text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = labels, # here each document can be given a label
progressbar = FALSE)
# create vocabulary
vocab = create_vocabulary(iterator
, stopwords = tm::stopwords() # use stopwords from tm package
, ngram = c(1, 2) # adds all possible 2-grams
)
vocab
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 2
, doc_proportion_min = 0.001
, doc_proportion_max = 0.5)
vocab_pruned
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab_pruned)
# create document term matrix (sparse)
doc_term_mat = create_dtm(iterator, vectorizer)
dim(doc_term_mat)
doc_term_mat
# create tfidf matrix
tfidf_function <- TfIdf$new()
tfidf <- fit_transform(doc_term_mat, tfidf_function) # fit and transform in one step
tfidf
# collocations
collocation_model <- Collocations$new()
collocation_model$fit(iterator, n_iter = 2) # repeats 2 times to get 3 word collocations
# inspect collocation object
collocation_model$collocation_stat
text <- readLines('~/Dropbox/Data_Projects/Fake_News/data/text_input.txt')
# define preprocession function and tokenizer
prep_fun = tolower
tok_fun = word_tokenizer
# create iterator function
iterator = itoken(text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = labels, # here each document can be given a label
progressbar = FALSE)
library(text2vec)
# define preprocession function and tokenizer
prep_fun = tolower
tok_fun = word_tokenizer
# create iterator function
iterator = itoken(text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = labels, # here each document can be given a label
progressbar = FALSE)
# create vocabulary
vocab = create_vocabulary(iterator
, stopwords = tm::stopwords() # use stopwords from tm package
, ngram = c(1, 2) # adds all possible 2-grams
)
vocab
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 2
, doc_proportion_min = 0.001
, doc_proportion_max = 0.5)
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab_pruned)
# create document term matrix (sparse)
doc_term_mat = create_dtm(iterator, vectorizer)
dim(doc_term_mat)
# create tfidf matrix
tfidf_function <- TfIdf$new()
tfidf <- fit_transform(doc_term_mat, tfidf_function) # fit and transform in one step
# collocations ------------
# collocation models
collocation_model <- Collocations$new()
collocation_model$fit(iterator, n_iter = 2) # repeats 2 times to get 3 word collocations
# inspect collocation object
collocation_model$collocation_stat
# collocations ------------
# collocation models
collocation_model <- Collocations$new(
collocation_count_min = 50 # only consider set of words as phrase if it will observe it at least n times
, pmi_min = 5
, lfmd_min = -Inf
, gensim_min = 0
)
collocation_model$fit(iterator, n_iter = 5) # repeats 2 times to get 3 word collocations
# inspect collocation object
collocation_model$collocation_stat
temp <- collocation_model$transform(iterator)
temp
install.packages('tokenizers')
tokenizers::stopwords()
tm::stopwords()
iterator
iterator$nextElem()
iterator$nextElem()$ids
temp <- iterator$nextElem()
temp$tokens[[1]]
temp$tokens[1]
temp$tokens[2]
temp$tokens[616]
class(temp$tokens)
temp$tokens[[1]]
head(temp$tokens)
?Collocations
# create vocabulary
vocab = create_vocabulary(iterator
, stopwords = tm::stopwords() # use stopwords from tm package
, ngram = c(1) # adds all possible 2-grams
)
# create vocabulary
vocab = create_vocabulary(iterator
, stopwords = tm::stopwords() # use stopwords from tm package
#, ngram = c(1, 2) # adds all possible 2-grams
)
vocab
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 2
, doc_proportion_min = 0.001
, doc_proportion_max = 0.5)
vocab_pruned
# vocabulary can be pruned
vocab_pruned <- prune_vocabulary(vocab
, term_count_min = 50
, doc_proportion_min = 0.001
, doc_proportion_max = 0.5)
vocab_pruned
# create a vectorizer function
vectorizer <- vocab_vectorizer(vocab_pruned)
# collocations ------------
# collocation models
collocation_model <- Collocations$new(
collocation_count_min = 50 # only consider set of words as phrase if it will observe it at least n times
, pmi_min = 5
, lfmd_min = -Inf
, gensim_min = 0
)
collocation_model$fit(iterator, n_iter = 5) # repeats 2 times to get 3 word collocations
# inspect collocation object
collocation_model$collocation_stat
temp
# apply collocation_model on new data set
temp <- collocation_model$transform(iterator)
temp
# create new iterator by applying collocation_model on new data set
iterator_w_collocations <- collocation_model$transform(iterator)
iterator$nextElem()$tokens[[1]] # vector of 1st documents tokens
iterator_w_collocations$nextElem()$tokens[[1]]
# use this to create new vocabulary
vocab_w_collocations <- create_vocabulary(iterator_w_collocations)
vocab_w_collocations
vocab_w_collocations[startsWith(vocab_w_collocations$term, 'alfa')]
vocab_w_collocations[startsWith(vocab_w_collocations$term, 'alfa'),]
vocab_w_collocations[startsWith(vocab_w_collocations$term, 'alfa_'),]
vocab_w_collocations[startsWith(vocab_w_collocations$term, 'new_'),]
tcm = create_tcm(iterator_w_collocations, vocab_vectorizer(vocab_w_collocations))
tcm
glove = Glove$new(50, vocabulary = vocab_w_collocations, x_max = 50)
glove = GloVe$new(50, vocabulary = vocab_w_collocations, x_max = 50)
temp = glove$fit_transform(tcm, 10)
temp
class(temp)
dim(temp)
wv_context = glove$components
word_vectors = temp + t(wv_context)
cos_sim = sim2(x = word_vectors, y = word_vectors['mann',,drop = F], method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
cos_sim = sim2(x = word_vectors, y = word_vectors['kaffee',,drop = F], method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
cos_sim = sim2(x = word_vectors, y = word_vectors['berlin',,drop = F], method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
outpath <- '/Users/Samuel/Dropbox/Data_Projects/Fake_News/'
library(xml2)
library(tidyr)
library(dplyr)
library(purrr)
library.install('tidyverse')
install.packages('tidyverse')
library(xml2)
library(tidyr)
library(dplyr)
library(purrr)
outpath
outpath <- '/Users/Samuel/Dropbox/Data_Projects/Fake_News/WebScraping/'
load(paste0(outpath,'input.Rda'))
load(paste0(outpath,'input.Rda'))
outpath <- '/Users/Samuel/Dropbox/Data_Projects/Fake_News/data/'
load(paste0(outpath,'input.Rda'))
# export
text_file <- file(paste0(outpath, 'text_input.txt'))
writeLines(paste(text, collapse = '\n'), text_file)
close(text_file)
title_file <- file(paste0(outpath, 'title_input.txt'))
writeLines(paste(title, collapse = '\n'), title_file)
close(title_file)
library(keras)
use_python('/Users/Samuel/VirtualenvTensorFlow/bin/python', required = TRUE)
py_config()
library(reticulate)
py_config()
use_python('/Users/Samuel/VirtualenvTensorFlow/bin/python', required = TRUE)
py_config()
?use_virtualenv
use_virtualenv('/Users/Samuel/VirtualenvTensorFlow', required = TRUE)
py_config()
library(reticulate)
use_virtualenv('/Users/Samuel/VirtualenvTensorFlow', required = TRUE)
py_config()
?initialize_python
props <- lapply(data_raw$features, function(x) x[['properties']]) %>% as_tibble()
library(reticulate)
py_config()
library(reticulate)
py_config()
library(reticulate)
use_virtualenv('/Users/Samuel/VirtualenvTensorFlow', required = TRUE)
py_config()
library(reticulate)
use_virtualenv('/Users/Samuel/VirtualenvTensorFlow', required = TRUE)
py_config()
library(reticulate)
use_virtualenv('/Users/Samuel/VirtualenvKeras', required = TRUE)
use_virtualenv('/Users/Samuel/VirtualenvKeras', required = TRUE)
library(reticulate)
use_virtualenv('/Users/Samuel/VirtualenvKeras', required = TRUE)
use_virtualenv('/Users/Samuel/VirtualenvTensorFlow', required = TRUE)
py_config()
use_virtualenv('/Users/Samuel/VirtualenvKeras', required = TRUE)
virtualenv_list()
install.packages('jsonlite')
library(reticulate)
use_python('/usr/local/Cellar/python3/3.7.0/bin')
py_config()
use_python('/usr/local/Cellar/python3/3.7.0/bin', required = TRUE)
py_config()
library(reticulate)
use_python('/usr/local/Cellar/python3/3.7.0/bin', required = TRUE)
py_config()
?use_python
library(reticulate)
use_virtualenv('~/VirtualenvKeras', require = TRUE)
py_config()
library(shiny)
library(shinythemes)
library(jsonlite)
library(tidyverse)
library(reticulate)
# set working directory
setwd("~/Dropbox/Data_Projects/SDD_Hackathon")
###################################################################################################
# data download & preparation
###################################################################################################
# read url and convert to data.frame
url <- 'https://data.stadt-zuerich.ch/dataset/zueriwieneu_meldungen/resource/2fee5562-1842-4ccc-a390-c52c9dade90d/download/zueriwieneu_meldungen.json'
dataZWN <- fromJSON(txt=url)
dataZWN_df<-dataZWN$features$properties %>% as_tibble()
# rename categories
data_total <- mutate(dataZWN_df
, service_name = case_when(service_name == 'Abfall/Sammelstelle' ~ 'Cat1',
service_name == 'Strasse/Trottoir/Platz' ~ 'Cat2',
service_name == 'Signalisation/Lichtsignal' ~ 'Cat3',
service_name == 'Grünflächen/Spielplätze' ~ 'Cat4',
service_name == 'Beleuchtung/Uhren' ~ 'Cat5',
service_name == 'Graffiti' ~ 'Cat6',
service_name == 'VBZ/ÖV' ~ 'Cat7',
service_name == 'Brunnen/Hydranten' ~ 'Cat8',
TRUE ~ 'Error'))
###################################################################################################
# prediction function
###################################################################################################
# connect to the desired python installation
use_python('/usr/local/Cellar/python/3.7.0/bin/python3', required = TRUE)
py_config() # check if it has connected to the correct version
# source python script
source_python('python_presentation_model.py')
# tain model on full dataset
presentation_models <- train_model(data_total$detail, data_total$service_name)
# prediction function
predict_category <- function(text){
pred <- py_predict(c(text, text), presentation_models)[1,]
cat <- which.max(pred)
return(paste(case_when(cat == 1 ~ 'Abfall/Sammelstelle',
cat == 2 ~  'Strasse/Trottoir/Platz',
cat == 3 ~  'Signalisation/Lichtsignal',
cat == 4 ~  'Grünflächen/Spielplätze',
cat == 5 ~  'Beleuchtung/Uhren',
cat == 6 ~  'Graffiti',
cat == 7 ~  'VBZ/ÖV',
cat == 8 ~  'Brunnen/Hydranten',
TRUE ~ 'No comment on this one...'),sprintf('%1.1f%%',100*max(pred))))
}
# user interface
ui <- fluidPage(
bootstrapPage(theme = shinytheme('superhero'),
fluidRow(
column(12,
align = 'center',
titlePanel('Swiss Digital Day 2018 Hackathon'),
h3('Züri Wie Neu Challenge'), br(), br(),
img('', src = 'Logo_128.png'), br(), br(),
textInput(inputId = 'text', label = 'Bitte eine Beschreibung des Schadens angeben'),
textOutput('cat')
)
)
)
)
# server
server <- function(input, output){
output$cat <- renderText(if(input$text == ''){''}else{predict_category(input$text)})
}
# run app
shinyApp(ui, server)
